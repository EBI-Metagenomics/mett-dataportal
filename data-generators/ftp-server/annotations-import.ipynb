{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ftplib\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import psycopg\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='process_gff.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s:%(message)s'\n",
    ")\n",
    "\n",
    "# Mapping task file path\n",
    "mapping_task_file = './gff-assembly-prefixes.tsv'\n",
    "\n",
    "# Load the mapping task file into a dictionary\n",
    "isolate_to_assembly_map = {}\n",
    "with open(mapping_task_file, mode='r') as file:\n",
    "    reader = csv.DictReader(file, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        # Remove the \".fa\" suffix from the assembly name\n",
    "        assembly_name = row['assembly'].replace('.fa', '')\n",
    "        isolate_to_assembly_map[row['prefix']] = assembly_name\n",
    "\n",
    "# FTP server details\n",
    "ftp_server = 'ftp.ebi.ac.uk'\n",
    "ftp_directory = '/pub/databases/mett/annotations/v1_2024-04-15/'\n",
    "\n",
    "# Query to insert gene\n",
    "gene_insert_query = \"\"\"\n",
    "INSERT INTO Gene (strain_id, gene_name, locus_tag, description, annotations)\n",
    "VALUES (%s, %s, %s, %s, %s::jsonb)\n",
    "ON CONFLICT (locus_tag) DO NOTHING;\n",
    "\"\"\"\n",
    "\n",
    "# Query to insert ontology term\n",
    "ontology_insert_query = \"\"\"\n",
    "INSERT INTO Gene_Ontology_Term (gene_id, ontology_type, ontology_id, ontology_description)\n",
    "VALUES (%s, %s, %s, %s)\n",
    "ON CONFLICT DO NOTHING;\n",
    "\"\"\"\n",
    "\n",
    "# Query to update the Strain table with the GFF file path\n",
    "update_strain_query = \"\"\"\n",
    "UPDATE Strain\n",
    "SET gff_file = %s\n",
    "WHERE assembly_name = %s\n",
    "\"\"\"\n",
    "\n",
    "# Function to list files in the directory and subdirectories\n",
    "def list_files(ftp, path):\n",
    "    try:\n",
    "        file_list = ftp.nlst(path)\n",
    "        logging.info(f\"Files listed in {path}: {file_list}\")\n",
    "        return file_list\n",
    "    except ftplib.error_perm as resp:\n",
    "        if \"No files found\" in str(resp):\n",
    "            logging.warning(f\"No files found in {path}\")\n",
    "            return []\n",
    "        else:\n",
    "            logging.error(f\"FTP error listing files in {path}: {resp}\")\n",
    "            raise\n",
    "\n",
    "# Function to process a single GFF file\n",
    "def process_gff_file(gff_file, isolate, strain_id):\n",
    "    logging.info(f\"Starting processing of {gff_file} for isolate {isolate}\")\n",
    "    try:\n",
    "        with ftplib.FTP(ftp_server) as ftp:\n",
    "            ftp.login()\n",
    "            # Download the GFF file\n",
    "            local_gff_path = os.path.join('/tmp', os.path.basename(gff_file))\n",
    "            with open(local_gff_path, 'wb') as f:\n",
    "                ftp.retrbinary(f\"RETR {gff_file}\", f.write)\n",
    "            logging.info(f\"Downloaded {gff_file} to {local_gff_path}\")\n",
    "\n",
    "        genes_to_insert = []\n",
    "        ontology_terms = []\n",
    "\n",
    "        with open(local_gff_path, 'r') as gff:\n",
    "            sequence_started = False  # Flag to track if we're in the sequence section\n",
    "            for line in gff:\n",
    "                if line.startswith(\"##FASTA\"):\n",
    "                    logging.info(f\"Ignoring assembly sequence in {gff_file}\")\n",
    "                    sequence_started = True\n",
    "                    break  # Stop reading the file when assembly sequence starts\n",
    "\n",
    "                if line.startswith(\"#\"):\n",
    "                    continue\n",
    "                columns = line.strip().split(\"\\t\")\n",
    "                if len(columns) != 9:\n",
    "                    logging.warning(f\"Skipping malformed line in {gff_file}: {line}\")\n",
    "                    continue\n",
    "\n",
    "                seq_id, source, feature_type, start, end, score, strand, phase, attributes = columns\n",
    "                if feature_type != 'gene':\n",
    "                    continue\n",
    "                attr_dict = dict(item.split('=') for item in attributes.split(';') if '=' in item)\n",
    "                gene_name = attr_dict.get('Name')\n",
    "                locus_tag = attr_dict.get('locus_tag')\n",
    "                description = attr_dict.get('product')\n",
    "\n",
    "                if not locus_tag:\n",
    "                    logging.warning(f\"Skipping gene without locus_tag in {gff_file}\")\n",
    "                    continue\n",
    "\n",
    "                annotations = {\n",
    "                    \"seq_id\": seq_id,\n",
    "                    \"start_position\": start,\n",
    "                    \"end_position\": end,\n",
    "                    \"strand\": strand,\n",
    "                    \"attributes\": attr_dict\n",
    "                }\n",
    "                annotations_json = json.dumps(annotations)\n",
    "\n",
    "                genes_to_insert.append((strain_id[0], gene_name, locus_tag, description, annotations_json))\n",
    "\n",
    "                # Handle ontology terms\n",
    "                if 'interpro' in attr_dict:\n",
    "                    ontology_ids = attr_dict['interpro'].split(',')\n",
    "                    for ont_id in ontology_ids:\n",
    "                        ontology_terms.append((locus_tag, 'InterPro', ont_id.strip(), None))\n",
    "\n",
    "        os.remove(local_gff_path)\n",
    "        logging.info(f\"Removed local GFF file {local_gff_path}\")\n",
    "\n",
    "        # Insert genes and ontology terms into the database\n",
    "        with psycopg.connect(\n",
    "            dbname=\"postgres\",\n",
    "            user=\"postgres\",\n",
    "            password=\"pass123\",\n",
    "            host=\"localhost\",\n",
    "            port=\"5432\"\n",
    "        ) as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                if genes_to_insert:\n",
    "                    # Batch insert genes\n",
    "                    cursor.executemany(gene_insert_query, genes_to_insert)\n",
    "                    logging.info(f\"Inserted {len(genes_to_insert)} genes from {gff_file}\")\n",
    "                else:\n",
    "                    logging.warning(f\"No genes to insert from {gff_file}\")\n",
    "\n",
    "                # Insert ontology terms\n",
    "                for gene_id_tuple, ontology_type, ontology_id, ontology_description in ontology_terms:\n",
    "                    cursor.execute(\n",
    "                        \"SELECT id FROM Gene WHERE locus_tag = %s\",\n",
    "                        (gene_id_tuple,)\n",
    "                    )\n",
    "                    gene_id = cursor.fetchone()\n",
    "                    if gene_id:\n",
    "                        cursor.execute(ontology_insert_query, (gene_id[0], ontology_type, ontology_id, ontology_description))\n",
    "                logging.info(f\"Inserted ontology terms for {gff_file}\")\n",
    "\n",
    "                # Update Strain table\n",
    "                assembly_name = isolate_to_assembly_map.get(isolate)\n",
    "                cursor.execute(update_strain_query, (os.path.basename(gff_file), assembly_name))\n",
    "                logging.info(f\"Updated Strain table with {os.path.basename(gff_file)} for assembly: {assembly_name}\")\n",
    "\n",
    "            conn.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing GFF file {gff_file}: {e}\", exc_info=True)\n",
    "\n",
    "# Main function to process isolates\n",
    "def process_isolate(isolate):\n",
    "    logging.info(f\"Processing isolate: {isolate}\")\n",
    "    # Find the corresponding assembly name from the mapping file\n",
    "    assembly_name = isolate_to_assembly_map.get(isolate)\n",
    "    try:\n",
    "        with ftplib.FTP(ftp_server) as ftp:\n",
    "            ftp.login()\n",
    "            isolate_path = f\"{ftp_directory}/{isolate}/functional_annotation/merged_gff/\"\n",
    "            gff_files_in_isolate = list_files(ftp, isolate_path)\n",
    "\n",
    "        if not gff_files_in_isolate:\n",
    "            logging.warning(f\"No GFF files found for isolate {isolate}\")\n",
    "            return\n",
    "\n",
    "        # Get the strain_id from the database\n",
    "        with psycopg.connect(\n",
    "            dbname=\"postgres\",\n",
    "            user=\"postgres\",\n",
    "            password=\"pass123\",\n",
    "            host=\"localhost\",\n",
    "            port=\"5432\"\n",
    "        ) as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(\"SELECT id FROM Strain WHERE assembly_name = %s\", (assembly_name,))\n",
    "                strain_id = cursor.fetchone()\n",
    "\n",
    "                if not strain_id:\n",
    "                    logging.error(f\"No strain_id found for isolate: {isolate}\")\n",
    "                    return\n",
    "\n",
    "        # Use ThreadPoolExecutor for I/O-bound parallelism\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            futures = []\n",
    "            for gff_file in gff_files_in_isolate:\n",
    "                futures.append(executor.submit(process_gff_file, gff_file, isolate, strain_id))\n",
    "\n",
    "            for future in futures:\n",
    "                future.result()  # Wait for all threads to complete\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing isolate {isolate}: {e}\", exc_info=True)\n",
    "\n",
    "# Main entry point\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        with ftplib.FTP(ftp_server) as ftp:\n",
    "            ftp.login()\n",
    "            ftp.cwd(ftp_directory)\n",
    "            isolates = ftp.nlst()\n",
    "            logging.info(f\"Found isolates: {isolates}\")\n",
    "\n",
    "        # Use ThreadPoolExecutor to process multiple isolates in parallel\n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            futures = [\n",
    "                executor.submit(process_isolate, isolate)\n",
    "                for isolate in isolates\n",
    "            ]\n",
    "            for future in futures:\n",
    "                future.result()  # Wait for all isolates to be processed\n",
    "\n",
    "        logging.info(\"GFF files processed, and strain table updated.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main processing: {e}\", exc_info=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
